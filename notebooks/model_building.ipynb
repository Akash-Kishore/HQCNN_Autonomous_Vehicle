{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fd51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Infrastructure: Connected to 'src' successfully.\n",
      "ðŸš€ System Check: Training on cuda\n",
      "   GPU Model: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- PATH SETUP ---\n",
    "# Links the notebook to your 'src' folder\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# --- IMPORTS ---\n",
    "try:\n",
    "    from src.hqcnn_model import HQCNN\n",
    "    from src.data_loader import get_data_loaders\n",
    "    print(\"âœ… Infrastructure: Connected to 'src' successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Stop! Please ensure your src/ folder files are populated.\")\n",
    "\n",
    "# --- GPU CHECK ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ System Check: Training on {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU Model: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Loading Full Dataset (to be decimated)...\n",
      "Dataset Loaded: 33327 Train | 5882 Val\n",
      "\n",
      "âœ‚ï¸ Slicing Data to 10%...\n",
      "----------------------------------------\n",
      "âœ… RESEARCH MODE: 10% DATA ACTIVATED\n",
      "----------------------------------------\n",
      "   Training Images:  3332 (Speed: ~25 mins/epoch)\n",
      "   Validation Images: 588\n",
      "----------------------------------------\n",
      "ðŸ§  HQCNN Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "# --- 1. HYPERPARAMETERS (Optimized for 10% Data) ---\n",
    "BATCH_SIZE = 50        \n",
    "EPOCHS = 35            \n",
    "LEARNING_RATE = 0.001  \n",
    "DATA_DIR = '../data/GTSRB'\n",
    "\n",
    "# --- 2. LOAD FULL DATA ---\n",
    "print(\"â³ Loading Full Dataset (to be decimated)...\")\n",
    "full_train_loader, full_val_loader = get_data_loaders(DATA_DIR, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- 3. âœ‚ï¸ THE 10% DECIMATION ---\n",
    "def get_subset(loader, fraction=0.10):\n",
    "    dataset = loader.dataset\n",
    "    num_samples = len(dataset)\n",
    "    split = int(np.floor(fraction * num_samples))\n",
    "    \n",
    "    # Random Seed 42 = Reproducible Science for your Paper\n",
    "    indices = list(range(num_samples))\n",
    "    np.random.seed(42) \n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Slice the top 10%\n",
    "    subset_indices = indices[:split]\n",
    "    \n",
    "    return Subset(dataset, subset_indices)\n",
    "\n",
    "print(f\"\\nâœ‚ï¸ Slicing Data to 10%...\")\n",
    "train_subset = get_subset(full_train_loader, fraction=0.10)\n",
    "val_subset = get_subset(full_val_loader, fraction=0.10)\n",
    "\n",
    "# Create NEW DataLoaders\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- 4. VERIFICATION ---\n",
    "print(\"-\" * 40)\n",
    "print(f\"âœ… RESEARCH MODE: 10% DATA ACTIVATED\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Training Images:  {len(train_loader.dataset)} (Speed: ~25 mins/epoch)\")\n",
    "print(f\"   Validation Images: {len(val_loader.dataset)}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 5. INITIALIZE MODEL ---\n",
    "model = HQCNN(n_classes=43).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(\"ðŸ§  HQCNN Model Initialized.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4532f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignite_training(num_epochs):\n",
    "    print(f\"\\nðŸ”¥ IGNITION: Starting Full Hybrid Training ({num_epochs} Epochs)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0.0\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass (Quantum Simulation happens here)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # ðŸ’“ HEARTBEAT: Print status every 10 batches (approx 45s)\n",
    "            # This keeps you informed that the GPU is working\n",
    "            if (i + 1) % 10 == 0:\n",
    "                batch_acc = 100 * (predicted == labels).sum().item() / labels.size(0)\n",
    "                print(f\"   Batch [{i+1}/{total_batches}] | Loss: {loss.item():.4f} | Acc: {batch_acc:.1f}%\")\n",
    "\n",
    "        # --- END OF EPOCH ---\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation Phase\n",
    "        print(\"   â³ Validating...\")\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss_accum = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_accum += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss_accum / len(val_loader)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"âœ… Epoch {epoch+1} Complete | Time: {epoch_time/60:.1f} min\")\n",
    "        print(f\"   Results: Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Store History\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save Best Model (Critical for hitting 99.63%)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), '../models/best_hqcnn.pth')\n",
    "            print(f\"   ðŸ’¾ New Best Model Saved ({val_acc:.2f}%)\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nðŸ† Training Complete in {total_time/3600:.2f} hours.\")\n",
    "    print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052954a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¥ IGNITION: Starting Full Hybrid Training (35 Epochs)...\n",
      "\n",
      "--- Epoch 1/35 ---\n",
      "   Batch [10/67] | Loss: 3.7331 | Acc: 2.0%\n",
      "   Batch [20/67] | Loss: 3.7090 | Acc: 2.0%\n",
      "   Batch [30/67] | Loss: 3.6432 | Acc: 6.0%\n",
      "   Batch [40/67] | Loss: 3.4896 | Acc: 2.0%\n",
      "   Batch [50/67] | Loss: 3.5541 | Acc: 6.0%\n",
      "   Batch [60/67] | Loss: 3.4669 | Acc: 0.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 1 Complete | Time: 39.1 min\n",
      "   Results: Train Acc: 4.62% | Val Acc: 5.61% | Val Loss: 3.4537\n",
      "   ðŸ’¾ New Best Model Saved (5.61%)\n",
      "\n",
      "--- Epoch 2/35 ---\n",
      "   Batch [10/67] | Loss: 3.4228 | Acc: 2.0%\n",
      "   Batch [20/67] | Loss: 3.2572 | Acc: 0.0%\n",
      "   Batch [30/67] | Loss: 3.5578 | Acc: 6.0%\n",
      "   Batch [40/67] | Loss: 3.6139 | Acc: 6.0%\n",
      "   Batch [50/67] | Loss: 3.3866 | Acc: 12.0%\n",
      "   Batch [60/67] | Loss: 3.4102 | Acc: 4.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 2 Complete | Time: 40.3 min\n",
      "   Results: Train Acc: 5.52% | Val Acc: 5.61% | Val Loss: 3.4519\n",
      "\n",
      "--- Epoch 3/35 ---\n",
      "   Batch [10/67] | Loss: 3.4793 | Acc: 12.0%\n",
      "   Batch [20/67] | Loss: 3.4189 | Acc: 8.0%\n",
      "   Batch [30/67] | Loss: 3.4230 | Acc: 2.0%\n",
      "   Batch [40/67] | Loss: 3.5557 | Acc: 8.0%\n",
      "   Batch [50/67] | Loss: 3.4766 | Acc: 8.0%\n",
      "   Batch [60/67] | Loss: 3.5179 | Acc: 2.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 3 Complete | Time: 40.6 min\n",
      "   Results: Train Acc: 6.15% | Val Acc: 6.12% | Val Loss: 3.4505\n",
      "   ðŸ’¾ New Best Model Saved (6.12%)\n",
      "\n",
      "--- Epoch 4/35 ---\n",
      "   Batch [10/67] | Loss: 3.3532 | Acc: 4.0%\n",
      "   Batch [20/67] | Loss: 3.5682 | Acc: 4.0%\n",
      "   Batch [30/67] | Loss: 3.5721 | Acc: 4.0%\n",
      "   Batch [40/67] | Loss: 3.5668 | Acc: 6.0%\n",
      "   Batch [50/67] | Loss: 3.3512 | Acc: 6.0%\n",
      "   Batch [60/67] | Loss: 3.5384 | Acc: 6.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 4 Complete | Time: 39.4 min\n",
      "   Results: Train Acc: 6.03% | Val Acc: 7.82% | Val Loss: 3.4472\n",
      "   ðŸ’¾ New Best Model Saved (7.82%)\n",
      "\n",
      "--- Epoch 5/35 ---\n",
      "   Batch [10/67] | Loss: 3.3228 | Acc: 2.0%\n",
      "   Batch [20/67] | Loss: 3.4959 | Acc: 6.0%\n",
      "   Batch [30/67] | Loss: 3.4509 | Acc: 8.0%\n",
      "   Batch [40/67] | Loss: 3.3287 | Acc: 10.0%\n",
      "   Batch [50/67] | Loss: 3.3282 | Acc: 14.0%\n",
      "   Batch [60/67] | Loss: 3.6187 | Acc: 14.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 5 Complete | Time: 39.4 min\n",
      "   Results: Train Acc: 5.64% | Val Acc: 5.61% | Val Loss: 3.4469\n",
      "\n",
      "--- Epoch 6/35 ---\n",
      "   Batch [10/67] | Loss: 3.5527 | Acc: 6.0%\n",
      "   Batch [20/67] | Loss: 3.5919 | Acc: 6.0%\n",
      "   Batch [30/67] | Loss: 3.4531 | Acc: 4.0%\n",
      "   Batch [40/67] | Loss: 3.4446 | Acc: 8.0%\n",
      "   Batch [50/67] | Loss: 3.4034 | Acc: 6.0%\n",
      "   Batch [60/67] | Loss: 3.4994 | Acc: 16.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 6 Complete | Time: 39.9 min\n",
      "   Results: Train Acc: 6.99% | Val Acc: 7.14% | Val Loss: 3.4402\n",
      "\n",
      "--- Epoch 7/35 ---\n",
      "   Batch [10/67] | Loss: 3.5183 | Acc: 4.0%\n",
      "   Batch [20/67] | Loss: 3.5851 | Acc: 6.0%\n",
      "   Batch [30/67] | Loss: 3.5677 | Acc: 6.0%\n",
      "   Batch [40/67] | Loss: 3.4367 | Acc: 8.0%\n",
      "   Batch [50/67] | Loss: 3.6441 | Acc: 4.0%\n",
      "   Batch [60/67] | Loss: 3.4241 | Acc: 10.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 7 Complete | Time: 39.2 min\n",
      "   Results: Train Acc: 7.44% | Val Acc: 8.33% | Val Loss: 3.4315\n",
      "   ðŸ’¾ New Best Model Saved (8.33%)\n",
      "\n",
      "--- Epoch 8/35 ---\n",
      "   Batch [10/67] | Loss: 3.5439 | Acc: 2.0%\n",
      "   Batch [20/67] | Loss: 3.5732 | Acc: 8.0%\n",
      "   Batch [30/67] | Loss: 3.3140 | Acc: 8.0%\n",
      "   Batch [40/67] | Loss: 3.5380 | Acc: 8.0%\n",
      "   Batch [50/67] | Loss: 3.3609 | Acc: 14.0%\n",
      "   Batch [60/67] | Loss: 3.2812 | Acc: 6.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 8 Complete | Time: 43.4 min\n",
      "   Results: Train Acc: 7.32% | Val Acc: 9.18% | Val Loss: 3.4188\n",
      "   ðŸ’¾ New Best Model Saved (9.18%)\n",
      "\n",
      "--- Epoch 9/35 ---\n",
      "   Batch [10/67] | Loss: 3.3938 | Acc: 2.0%\n",
      "   Batch [20/67] | Loss: 3.4457 | Acc: 6.0%\n",
      "   Batch [30/67] | Loss: 3.4930 | Acc: 14.0%\n",
      "   Batch [40/67] | Loss: 3.3444 | Acc: 12.0%\n",
      "   Batch [50/67] | Loss: 3.5712 | Acc: 4.0%\n",
      "   Batch [60/67] | Loss: 3.4925 | Acc: 6.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 9 Complete | Time: 46.7 min\n",
      "   Results: Train Acc: 8.28% | Val Acc: 9.01% | Val Loss: 3.3947\n",
      "\n",
      "--- Epoch 10/35 ---\n",
      "   Batch [10/67] | Loss: 3.4496 | Acc: 4.0%\n",
      "   Batch [20/67] | Loss: 3.4862 | Acc: 2.0%\n",
      "   Batch [30/67] | Loss: 3.3857 | Acc: 2.0%\n",
      "   Batch [40/67] | Loss: 3.3697 | Acc: 6.0%\n",
      "   Batch [50/67] | Loss: 3.3685 | Acc: 14.0%\n",
      "   Batch [60/67] | Loss: 3.4194 | Acc: 4.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 10 Complete | Time: 42.1 min\n",
      "   Results: Train Acc: 8.10% | Val Acc: 8.84% | Val Loss: 3.3737\n",
      "\n",
      "--- Epoch 11/35 ---\n",
      "   Batch [10/67] | Loss: 3.4167 | Acc: 12.0%\n",
      "   Batch [20/67] | Loss: 3.6642 | Acc: 4.0%\n",
      "   Batch [30/67] | Loss: 3.3052 | Acc: 12.0%\n",
      "   Batch [40/67] | Loss: 3.5756 | Acc: 6.0%\n",
      "   Batch [50/67] | Loss: 3.3261 | Acc: 10.0%\n",
      "   Batch [60/67] | Loss: 3.4197 | Acc: 6.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 11 Complete | Time: 40.5 min\n",
      "   Results: Train Acc: 8.37% | Val Acc: 10.03% | Val Loss: 3.3487\n",
      "   ðŸ’¾ New Best Model Saved (10.03%)\n",
      "\n",
      "--- Epoch 12/35 ---\n",
      "   Batch [10/67] | Loss: 3.3350 | Acc: 4.0%\n",
      "   Batch [20/67] | Loss: 3.5893 | Acc: 12.0%\n",
      "   Batch [30/67] | Loss: 3.3490 | Acc: 12.0%\n",
      "   Batch [40/67] | Loss: 3.4057 | Acc: 4.0%\n",
      "   Batch [50/67] | Loss: 3.3882 | Acc: 8.0%\n",
      "   Batch [60/67] | Loss: 3.3127 | Acc: 8.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 12 Complete | Time: 38.7 min\n",
      "   Results: Train Acc: 8.31% | Val Acc: 9.52% | Val Loss: 3.3278\n",
      "\n",
      "--- Epoch 13/35 ---\n",
      "   Batch [10/67] | Loss: 3.3097 | Acc: 10.0%\n",
      "   Batch [20/67] | Loss: 3.4030 | Acc: 8.0%\n",
      "   Batch [30/67] | Loss: 3.3291 | Acc: 12.0%\n",
      "   Batch [40/67] | Loss: 3.3788 | Acc: 6.0%\n",
      "   Batch [50/67] | Loss: 3.4388 | Acc: 2.0%\n",
      "   Batch [60/67] | Loss: 3.3673 | Acc: 2.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 13 Complete | Time: 38.7 min\n",
      "   Results: Train Acc: 8.91% | Val Acc: 10.03% | Val Loss: 3.3042\n",
      "\n",
      "--- Epoch 14/35 ---\n",
      "   Batch [10/67] | Loss: 3.4313 | Acc: 8.0%\n",
      "   Batch [20/67] | Loss: 3.7951 | Acc: 4.0%\n",
      "   Batch [30/67] | Loss: 3.4577 | Acc: 8.0%\n",
      "   Batch [40/67] | Loss: 3.2573 | Acc: 12.0%\n",
      "   Batch [50/67] | Loss: 3.3824 | Acc: 10.0%\n",
      "   Batch [60/67] | Loss: 3.3793 | Acc: 10.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 14 Complete | Time: 38.8 min\n",
      "   Results: Train Acc: 8.52% | Val Acc: 10.54% | Val Loss: 3.2762\n",
      "   ðŸ’¾ New Best Model Saved (10.54%)\n",
      "\n",
      "--- Epoch 15/35 ---\n",
      "   Batch [10/67] | Loss: 3.1742 | Acc: 12.0%\n",
      "   Batch [20/67] | Loss: 3.2217 | Acc: 12.0%\n",
      "   Batch [30/67] | Loss: 3.1972 | Acc: 18.0%\n",
      "   Batch [40/67] | Loss: 3.2439 | Acc: 10.0%\n",
      "   Batch [50/67] | Loss: 3.3217 | Acc: 10.0%\n",
      "   Batch [60/67] | Loss: 3.1724 | Acc: 16.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 15 Complete | Time: 38.7 min\n",
      "   Results: Train Acc: 9.21% | Val Acc: 12.59% | Val Loss: 3.2387\n",
      "   ðŸ’¾ New Best Model Saved (12.59%)\n",
      "\n",
      "--- Epoch 16/35 ---\n",
      "   Batch [10/67] | Loss: 3.4635 | Acc: 4.0%\n",
      "   Batch [20/67] | Loss: 3.3541 | Acc: 6.0%\n",
      "   Batch [30/67] | Loss: 3.3842 | Acc: 6.0%\n",
      "   Batch [40/67] | Loss: 3.3661 | Acc: 16.0%\n",
      "   Batch [50/67] | Loss: 3.1095 | Acc: 16.0%\n",
      "   Batch [60/67] | Loss: 3.5043 | Acc: 16.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 16 Complete | Time: 38.6 min\n",
      "   Results: Train Acc: 9.72% | Val Acc: 15.65% | Val Loss: 3.2122\n",
      "   ðŸ’¾ New Best Model Saved (15.65%)\n",
      "\n",
      "--- Epoch 17/35 ---\n",
      "   Batch [10/67] | Loss: 3.4850 | Acc: 8.0%\n",
      "   Batch [20/67] | Loss: 3.4797 | Acc: 10.0%\n",
      "   Batch [30/67] | Loss: 3.3554 | Acc: 14.0%\n",
      "   Batch [40/67] | Loss: 3.2945 | Acc: 12.0%\n",
      "   Batch [50/67] | Loss: 3.3223 | Acc: 8.0%\n",
      "   Batch [60/67] | Loss: 3.4541 | Acc: 2.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 17 Complete | Time: 38.5 min\n",
      "   Results: Train Acc: 9.45% | Val Acc: 16.67% | Val Loss: 3.1859\n",
      "   ðŸ’¾ New Best Model Saved (16.67%)\n",
      "\n",
      "--- Epoch 18/35 ---\n",
      "   Batch [10/67] | Loss: 3.3221 | Acc: 12.0%\n",
      "   Batch [20/67] | Loss: 3.3301 | Acc: 12.0%\n",
      "   Batch [30/67] | Loss: 3.3517 | Acc: 10.0%\n",
      "   Batch [40/67] | Loss: 3.2154 | Acc: 18.0%\n",
      "   Batch [50/67] | Loss: 3.5280 | Acc: 6.0%\n",
      "   Batch [60/67] | Loss: 3.3795 | Acc: 10.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 18 Complete | Time: 38.9 min\n",
      "   Results: Train Acc: 11.25% | Val Acc: 17.01% | Val Loss: 3.1666\n",
      "   ðŸ’¾ New Best Model Saved (17.01%)\n",
      "\n",
      "--- Epoch 19/35 ---\n",
      "   Batch [10/67] | Loss: 3.2753 | Acc: 10.0%\n",
      "   Batch [20/67] | Loss: 3.3887 | Acc: 10.0%\n",
      "   Batch [30/67] | Loss: 3.1051 | Acc: 20.0%\n",
      "   Batch [40/67] | Loss: 3.1812 | Acc: 10.0%\n",
      "   Batch [50/67] | Loss: 3.5423 | Acc: 6.0%\n",
      "   Batch [60/67] | Loss: 3.3048 | Acc: 10.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 19 Complete | Time: 38.9 min\n",
      "   Results: Train Acc: 10.92% | Val Acc: 16.50% | Val Loss: 3.1375\n",
      "\n",
      "--- Epoch 20/35 ---\n",
      "   Batch [10/67] | Loss: 3.3803 | Acc: 12.0%\n",
      "   Batch [20/67] | Loss: 3.1527 | Acc: 28.0%\n",
      "   Batch [30/67] | Loss: 3.1804 | Acc: 6.0%\n",
      "   Batch [40/67] | Loss: 3.2963 | Acc: 10.0%\n",
      "   Batch [50/67] | Loss: 3.0795 | Acc: 18.0%\n",
      "   Batch [60/67] | Loss: 3.3099 | Acc: 12.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 20 Complete | Time: 39.0 min\n",
      "   Results: Train Acc: 11.01% | Val Acc: 17.01% | Val Loss: 3.1192\n",
      "\n",
      "--- Epoch 21/35 ---\n",
      "   Batch [10/67] | Loss: 3.3215 | Acc: 8.0%\n",
      "   Batch [20/67] | Loss: 3.1151 | Acc: 18.0%\n",
      "   Batch [30/67] | Loss: 3.0291 | Acc: 12.0%\n",
      "   Batch [40/67] | Loss: 3.2997 | Acc: 14.0%\n",
      "   Batch [50/67] | Loss: 3.2881 | Acc: 10.0%\n",
      "   Batch [60/67] | Loss: 3.4831 | Acc: 6.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 21 Complete | Time: 38.8 min\n",
      "   Results: Train Acc: 10.80% | Val Acc: 16.84% | Val Loss: 3.0962\n",
      "\n",
      "--- Epoch 22/35 ---\n",
      "   Batch [10/67] | Loss: 3.0195 | Acc: 18.0%\n",
      "   Batch [20/67] | Loss: 3.1858 | Acc: 4.0%\n",
      "   Batch [30/67] | Loss: 3.5434 | Acc: 8.0%\n",
      "   Batch [40/67] | Loss: 3.1258 | Acc: 14.0%\n",
      "   Batch [50/67] | Loss: 3.1925 | Acc: 14.0%\n",
      "   Batch [60/67] | Loss: 3.2748 | Acc: 12.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 22 Complete | Time: 39.0 min\n",
      "   Results: Train Acc: 10.83% | Val Acc: 18.37% | Val Loss: 3.0823\n",
      "   ðŸ’¾ New Best Model Saved (18.37%)\n",
      "\n",
      "--- Epoch 23/35 ---\n",
      "   Batch [10/67] | Loss: 3.3020 | Acc: 6.0%\n",
      "   Batch [20/67] | Loss: 3.2122 | Acc: 16.0%\n",
      "   Batch [30/67] | Loss: 3.1949 | Acc: 8.0%\n",
      "   Batch [40/67] | Loss: 3.2935 | Acc: 4.0%\n",
      "   Batch [50/67] | Loss: 3.2161 | Acc: 14.0%\n",
      "   Batch [60/67] | Loss: 3.2840 | Acc: 12.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 23 Complete | Time: 39.0 min\n",
      "   Results: Train Acc: 11.16% | Val Acc: 16.67% | Val Loss: 3.0794\n",
      "\n",
      "--- Epoch 24/35 ---\n",
      "   Batch [10/67] | Loss: 3.1666 | Acc: 16.0%\n",
      "   Batch [20/67] | Loss: 3.0176 | Acc: 14.0%\n",
      "   Batch [30/67] | Loss: 3.0462 | Acc: 12.0%\n",
      "   Batch [40/67] | Loss: 3.2521 | Acc: 20.0%\n",
      "   Batch [50/67] | Loss: 3.1871 | Acc: 18.0%\n",
      "   Batch [60/67] | Loss: 3.2652 | Acc: 10.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 24 Complete | Time: 39.4 min\n",
      "   Results: Train Acc: 11.31% | Val Acc: 17.18% | Val Loss: 3.0589\n",
      "\n",
      "--- Epoch 25/35 ---\n",
      "   Batch [10/67] | Loss: 2.9719 | Acc: 12.0%\n",
      "   Batch [20/67] | Loss: 3.1638 | Acc: 8.0%\n",
      "   Batch [30/67] | Loss: 3.2365 | Acc: 16.0%\n",
      "   Batch [40/67] | Loss: 3.2480 | Acc: 10.0%\n",
      "   Batch [50/67] | Loss: 3.0842 | Acc: 16.0%\n",
      "   Batch [60/67] | Loss: 3.1556 | Acc: 8.0%\n",
      "   â³ Validating...\n",
      "âœ… Epoch 25 Complete | Time: 40.5 min\n",
      "   Results: Train Acc: 11.82% | Val Acc: 19.22% | Val Loss: 3.0377\n",
      "   ðŸ’¾ New Best Model Saved (19.22%)\n",
      "\n",
      "--- Epoch 26/35 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. RUN TRAINING\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This will take time! Monitor the heartbeat messages.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mignite_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 2. PLOT RESULTS\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mignite_training\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Forward Pass (Quantum Simulation happens here)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backward Pass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/HQCNN_Project/src/hqcnn_model.py:40\u001b[0m, in \u001b[0;36mHQCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m patches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m),   \u001b[38;5;66;03m# Top-Left\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# Top-Right\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# Bottom-Left\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m)  \u001b[38;5;66;03m# Bottom-Right\u001b[39;00m\n\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# QUANTUM PROCESSING\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run the circuit 4 times per image (once per patch)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m q_results \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantum_layer(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patches]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# FUSION\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Concatenate the 4 outputs: [Batch, 8] x 4 -> [Batch, 32]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(q_results, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/HQCNN_Project/src/hqcnn_model.py:40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m patches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     32\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m),   \u001b[38;5;66;03m# Top-Left\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# Top-Right\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m16\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# Bottom-Left\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     x[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m:\u001b[38;5;241m32\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m256\u001b[39m)  \u001b[38;5;66;03m# Bottom-Right\u001b[39;00m\n\u001b[1;32m     36\u001b[0m ]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# QUANTUM PROCESSING\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run the circuit 4 times per image (once per patch)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m q_results \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantum_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m patches]\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# FUSION\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Concatenate the 4 outputs: [Batch, 8] x 4 -> [Batch, 32]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(q_results, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/HQCNN_Project/src/quantum_circuit.py:26\u001b[0m, in \u001b[0;36mQuantumLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/qnn/torch.py:408\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    405\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/qnn/torch.py:434\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{arg: weight\u001b[38;5;241m.\u001b[39mto(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    433\u001b[0m }\n\u001b[0;32m--> 434\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/qnode.py:922\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_capture_qnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_qnode  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/qnode.py:895\u001b[0m, in \u001b[0;36mQNode._impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_program\u001b[38;5;241m.\u001b[39mset_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m--> 895\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/execution.py:233\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, diff_method, interface, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, transform_program, executor_backend)\u001b[0m\n\u001b[1;32m    229\u001b[0m tapes, outer_post_processing \u001b[38;5;241m=\u001b[39m outer_transform(tapes)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m outer_transform\u001b[38;5;241m.\u001b[39mis_informative, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould only contain device preprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_post_processing(outer_post_processing(results))\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/run.py:338\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(tapes, device, config, inner_transform_program)\u001b[0m\n\u001b[1;32m    335\u001b[0m         params \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    336\u001b[0m         tape\u001b[38;5;241m.\u001b[39mtrainable_params \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_trainable_indices(params)\n\u001b[0;32m--> 338\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mml_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/interfaces/torch.py:240\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, execute_fn, jpc, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mextend(tape\u001b[38;5;241m.\u001b[39mget_parameters())\n\u001b[1;32m    234\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtapes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(tapes),\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecute_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m: execute_fn,\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjpc\u001b[39m\u001b[38;5;124m\"\u001b[39m: jpc,\n\u001b[1;32m    238\u001b[0m }\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExecuteTapes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/interfaces/torch.py:89\u001b[0m, in \u001b[0;36mpytreeify.<locals>.new_apply\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_apply\u001b[39m(\u001b[38;5;241m*\u001b[39minp):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Inputs already flat\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     out_struct_holder \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 89\u001b[0m     flat_out \u001b[38;5;241m=\u001b[39m \u001b[43morig_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_struct_holder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pytree\u001b[38;5;241m.\u001b[39mtree_unflatten(flat_out, out_struct_holder[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/interfaces/torch.py:93\u001b[0m, in \u001b[0;36mpytreeify.<locals>.new_forward\u001b[0;34m(ctx, out_struct_holder, *inp)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(ctx, out_struct_holder, \u001b[38;5;241m*\u001b[39minp):\n\u001b[0;32m---> 93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43morig_fw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     flat_out, out_struct \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_flatten(out)\n\u001b[1;32m     95\u001b[0m     ctx\u001b[38;5;241m.\u001b[39m_out_struct \u001b[38;5;241m=\u001b[39m out_struct\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/interfaces/torch.py:162\u001b[0m, in \u001b[0;36mExecuteTapes.forward\u001b[0;34m(ctx, kwargs, *parameters)\u001b[0m\n\u001b[1;32m    159\u001b[0m ctx\u001b[38;5;241m.\u001b[39mtapes \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtapes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m ctx\u001b[38;5;241m.\u001b[39mjpc \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjpc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 162\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_fn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# if any input tensor uses the GPU, the output should as well\u001b[39;00m\n\u001b[1;32m    165\u001b[0m ctx\u001b[38;5;241m.\u001b[39mtorch_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/jacobian_products.py:487\u001b[0m, in \u001b[0;36mDeviceDerivatives.execute_and_cache_jacobian\u001b[0;34m(self, tapes)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logger\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward pass called with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, tapes)\n\u001b[0;32m--> 487\u001b[0m results, jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dev_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results_cache[tapes] \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jacs_cache[tapes] \u001b[38;5;241m=\u001b[39m jac\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/workflow/jacobian_products.py:452\u001b[0m, in \u001b[0;36mDeviceDerivatives._dev_execute_and_compute_derivatives\u001b[0;34m(self, tapes)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03mConverts tapes to numpy before computing the the results and derivatives on the device.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03mDispatches between the two different device interfaces.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m numpy_tapes, _ \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(tapes)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_tapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/devices/modifiers/simulator_tracking.py:95\u001b[0m, in \u001b[0;36m_track_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     90\u001b[0m         execute_and_derivative_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     91\u001b[0m         executions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[1;32m     92\u001b[0m         derivatives\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muntracked_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/devices/modifiers/single_tape_support.py:60\u001b[0m, in \u001b[0;36m_make_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     58\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[0;32m---> 60\u001b[0m results, jacs \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (results[\u001b[38;5;241m0\u001b[39m], jacs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m (results, jacs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/devices/modifiers/simulator_tracking.py:95\u001b[0m, in \u001b[0;36m_track_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     90\u001b[0m         execute_and_derivative_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     91\u001b[0m         executions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[1;32m     92\u001b[0m         derivatives\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(batch),\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muntracked_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane/devices/modifiers/single_tape_support.py:60\u001b[0m, in \u001b[0;36m_make_execute_and_compute_derivatives.<locals>.execute_and_compute_derivatives\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     58\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[0;32m---> 60\u001b[0m results, jacs \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (results[\u001b[38;5;241m0\u001b[39m], jacs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m (results, jacs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane_lightning/lightning_base/lightning_base.py:451\u001b[0m, in \u001b[0;36mLightningBase.execute_and_compute_derivatives\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the results and jacobians of circuits at the same time.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    Tuple: A numeric result of the computation and the gradient.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    450\u001b[0m batch_obs \u001b[38;5;241m=\u001b[39m execution_config\u001b[38;5;241m.\u001b[39mdevice_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_obs)\n\u001b[0;32m--> 451\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_and_jacobian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_wires_from_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statevector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwire_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wire_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults))\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane_lightning/lightning_base/lightning_base.py:452\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the results and jacobians of circuits at the same time.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    Tuple: A numeric result of the computation and the gradient.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    450\u001b[0m batch_obs \u001b[38;5;241m=\u001b[39m execution_config\u001b[38;5;241m.\u001b[39mdevice_options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_obs)\n\u001b[1;32m    451\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_and_jacobian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_wires_from_circuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statevector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwire_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wire_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m circuit \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults))\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane_lightning/lightning_base/lightning_base.py:342\u001b[0m, in \u001b[0;36mLightningBase.simulate_and_jacobian\u001b[0;34m(self, circuit, state, batch_obs, wire_map)\u001b[0m\n\u001b[1;32m    340\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulate(circuit, state)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLightningAdjointJacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_obs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_obs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res, jac\n",
      "File \u001b[0;32m~/miniconda3/envs/hqcnn_env/lib/python3.10/site-packages/pennylane_lightning/lightning_gpu/_adjoint_jacobian.py:161\u001b[0m, in \u001b[0;36mLightningGPUAdjointJacobian.calculate_jacobian\u001b[0;34m(self, tape)\u001b[0m\n\u001b[1;32m    154\u001b[0m     jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jacobian_lightning\u001b[38;5;241m.\u001b[39mbatched(\n\u001b[1;32m    155\u001b[0m         processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    156\u001b[0m         processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs_serialized\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    157\u001b[0m         processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mops_serialized\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    158\u001b[0m         trainable_params,\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jacobian_lightning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_vector\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs_serialized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mops_serialized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m jac \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(jac)\n\u001b[1;32m    169\u001b[0m has_shape0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mlen\u001b[39m(jac))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. RUN TRAINING\n",
    "# This will take time! Monitor the heartbeat messages.\n",
    "history = ignite_training(EPOCHS)\n",
    "\n",
    "# 2. PLOT RESULTS\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_acc'], label='Train Acc', color='blue')\n",
    "plt.plot(history['val_acc'], label='Val Acc', color='green')\n",
    "plt.axhline(y=99.63, color='r', linestyle='--', label='Target (99.63%)') # Target Line\n",
    "plt.title('Accuracy Trajectory')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train Loss', color='red')\n",
    "plt.plot(history['val_loss'], label='Val Loss', color='orange')\n",
    "plt.title('Loss Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hqcnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
